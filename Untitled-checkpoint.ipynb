{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import sklearn.preprocessing as pre_processing\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')\n",
    "\n",
    "#载入维度数据\n",
    "x_data = datasets[[ '吨位', '当日有派车次数',\n",
    "                   '全天行驶里程km', '其中任务行驶里程km', '全天运行时长（点火时长）hh', '其中任务行驶时长hh', '全天静驶时长',\n",
    "                   '平均时速km/h', '最大扭距', '平均扭距', '急刹车次数', '超速报警次数', '前3日平均运行里程km', '前3日日平均运行时长h',\n",
    "                   '近4日平均运行里程km', '近4日日平均运行时长h', '怠速次数', '怠速时长mm', '任务百公里油耗（L）', '全里程百公里油耗（L）'\n",
    "                   , '高档低速次数', '高档低速累计时长mm', '低档高速次数', '低档高速累计时长mm', '空档滑行次数', '空档滑行时长mm',\n",
    "                   'Can设备状态',  '市趟次数', '一干次数', '二干次数', '里程标杆值km',\n",
    "                  '实际里程参考值km', '位置总数', '未锁星数', '延迟位置数',\n",
    "                   '行驶位置数', '间隔大位置数', 'ACC关位置数',  '行驶时长', '拉直线次数', '拉直线总距离', \n",
    "                   '设备类型', '在线时长', '里程误差']]\n",
    "x_data = x_data.replace([np.inf, -np.inf], np.nan)\n",
    "x_data = x_data.fillna(0)\n",
    "\n",
    "#载入标签数据\n",
    "y_data = datasets['油耗量（当天）']\n",
    "\n",
    "#分割数据1/4为测试，3/4为训练数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import sklearn.preprocessing as pre_processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib.font_manager import *\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 44)\n",
      "(447, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_542 (Dense)            (None, 36)                1620      \n",
      "_________________________________________________________________\n",
      "dense_543 (Dense)            (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_544 (Dense)            (None, 5)                 125       \n",
      "_________________________________________________________________\n",
      "dense_545 (Dense)            (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_546 (Dense)            (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dense_547 (Dense)            (None, 20)                300       \n",
      "_________________________________________________________________\n",
      "dense_548 (Dense)            (None, 30)                630       \n",
      "_________________________________________________________________\n",
      "dense_549 (Dense)            (None, 44)                1364      \n",
      "=================================================================\n",
      "Total params: 4,981\n",
      "Trainable params: 4,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1005 samples, validate on 335 samples\n",
      "Epoch 1/50\n",
      "1005/1005 [==============================] - 1s 697us/step - loss: 0.0372 - val_loss: 0.0209\n",
      "Epoch 2/50\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0151 - val_loss: 0.0134\n",
      "Epoch 3/50\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 4/50\n",
      "1005/1005 [==============================] - 0s 262us/step - loss: 0.0128 - val_loss: 0.0123\n",
      "Epoch 5/50\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0125 - val_loss: 0.0122\n",
      "Epoch 6/50\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 7/50\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 8/50\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 9/50\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 10/50\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 11/50\n",
      "1005/1005 [==============================] - 0s 223us/step - loss: 0.0094 - val_loss: 0.0089\n",
      "Epoch 12/50\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 13/50\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0088 - val_loss: 0.0083\n",
      "Epoch 14/50\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0086 - val_loss: 0.0081\n",
      "Epoch 15/50\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 16/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0082 - val_loss: 0.0077\n",
      "Epoch 17/50\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 18/50\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 19/50\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 20/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 21/50\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 22/50\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 23/50\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 24/50\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 25/50\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 26/50\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 27/50\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0062 - val_loss: 0.0059\n",
      "Epoch 28/50\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 29/50\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 30/50\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0061 - val_loss: 0.0058\n",
      "Epoch 31/50\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 32/50\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 33/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 34/50\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 35/50\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 36/50\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 37/50\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 38/50\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 39/50\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 40/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 41/50\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 42/50\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 43/50\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 44/50\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 45/50\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 46/50\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 47/50\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 48/50\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 49/50\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0057 - val_loss: 0.0052\n",
      "Epoch 50/50\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0056 - val_loss: 0.0054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# please note, all tutorial code are running under python3.5.\n",
    "# If you use the version like python2.7, please modify the code accordingly\n",
    "\n",
    "# 9 - Autoencoder example\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# to try tensorflow, un-comment following two lines\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called\n",
    "# X shape (60,000 28x28), y shape (10,000, )\n",
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')\n",
    "datasets = datasets.drop(datasets.loc[(datasets['吨位'] == 0 )| (datasets['全天行驶里程km'] == 0 ) ].index)\n",
    "#载入维度数据\n",
    "x_data = datasets[[ '吨位', '当日有派车次数',\n",
    "                   '全天行驶里程km', '其中任务行驶里程km', '全天运行时长（点火时长）hh', '其中任务行驶时长hh', '全天静驶时长',\n",
    "                   '平均时速km/h', '最大扭距', '平均扭距', '急刹车次数', '超速报警次数', '前3日平均运行里程km', '前3日日平均运行时长h',\n",
    "                   '近4日平均运行里程km', '近4日日平均运行时长h', '怠速次数', '怠速时长mm', '任务百公里油耗（L）', '全里程百公里油耗（L）'\n",
    "                   , '高档低速次数', '高档低速累计时长mm', '低档高速次数', '低档高速累计时长mm', '空档滑行次数', '空档滑行时长mm',\n",
    "                   'Can设备状态',  '市趟次数', '一干次数', '二干次数', '里程标杆值km',\n",
    "                  '实际里程参考值km', '位置总数', '未锁星数', '延迟位置数',\n",
    "                   '行驶位置数', '间隔大位置数', 'ACC关位置数',  '行驶时长', '拉直线次数', '拉直线总距离', \n",
    "                   '设备类型', '在线时长', '里程误差']]\n",
    "x_data = x_data.replace([np.inf, -np.inf], np.nan)\n",
    "x_data = x_data.fillna(0)\n",
    "\n",
    "\n",
    "#载入标签数据\n",
    "y_data = datasets['油耗量（当天）']\n",
    "\n",
    "#分割数据1/4为测试，3/4为训练数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)\n",
    "\n",
    "\n",
    "#(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "'''\n",
    "# data pre-processing\n",
    "x_train = x_train.astype('float32') / 255. - 0.5       # minmax_normalized\n",
    "x_test = x_test.astype('float32') / 255. - 0.5         # minmax_normalized\n",
    "\n",
    "# standardize the feature 标准化\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# in order to plot in a 2D figure\n",
    "encoding_dim = 2\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(44,))\n",
    "\n",
    "# encoder layers\n",
    "encoded = Dense(36, activation='relu')(input_img)\n",
    "encoded = Dense(24, activation='relu')(encoded)\n",
    "encoded = Dense(5, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    "\n",
    "# decoder layers\n",
    "decoded = Dense(14, activation='relu')(encoder_output)\n",
    "decoded = Dense(20, activation='relu')(decoded)\n",
    "decoded = Dense(30, activation='relu')(decoded)\n",
    "decoded = Dense(44, activation='tanh')(decoded)\n",
    "\n",
    "# construct the autoencoder model\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "# construct the encoder model for plotting\n",
    "encoder = Model(input=input_img, output=encoder_output)\n",
    "\n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "print(autoencoder.summary())\n",
    "\n",
    "# training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split=0.25 )\n",
    "\n",
    "#print(history.history.keys()) # 可查看 history 对象中有哪些历史数据\n",
    "\n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1005 samples, validate on 335 samples\n",
      "Epoch 1/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 2/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 3/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 4/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 5/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 6/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 7/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 8/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 9/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 10/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 11/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 12/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 13/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 14/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 15/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0054 - val_loss: 0.0052\n",
      "Epoch 16/1050\n",
      "1005/1005 [==============================] - 0s 246us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 17/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 18/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 19/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 20/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 21/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 22/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 23/1050\n",
      "1005/1005 [==============================] - 0s 229us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 24/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 25/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 26/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 27/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 28/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 29/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 30/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 31/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 32/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 33/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 34/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 35/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 36/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 37/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 38/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 39/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 40/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 41/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 42/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 43/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 44/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 45/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 46/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 47/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 48/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 49/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 50/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 51/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 52/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 53/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 54/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 55/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 56/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 57/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 58/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 59/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 60/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 61/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 62/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 63/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 64/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 65/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 66/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 67/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 68/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 69/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 70/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 71/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 72/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 73/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 74/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 75/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 76/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 77/1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 78/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 79/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 80/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 81/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 82/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 83/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 84/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 85/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 86/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 87/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 88/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 89/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 90/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 91/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 92/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 93/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 94/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 95/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 96/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 97/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 98/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 99/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 100/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 101/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 102/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 103/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 104/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 105/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 106/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 107/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 108/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 109/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 110/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 111/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 112/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 113/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 114/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 115/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 116/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 117/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 118/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 119/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 120/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 121/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 122/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 123/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 124/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 125/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 126/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 127/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 128/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 129/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 130/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 131/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 132/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 133/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 134/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 135/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 136/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 137/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 138/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 139/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 140/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 141/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 142/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 143/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 144/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 145/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 146/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 147/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 148/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 149/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 150/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 151/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 152/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 153/1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 154/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 155/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 156/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 157/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 158/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 159/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 160/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 161/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 162/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 163/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 164/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 165/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 166/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 167/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 168/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 169/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 170/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 171/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 172/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 173/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 174/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 175/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 176/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 177/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 178/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 179/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 180/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 181/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 182/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 183/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 184/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 185/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 186/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 187/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 188/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 189/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 190/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 191/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 192/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 193/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 194/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 195/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 196/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 197/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 198/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 199/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 200/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 201/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 202/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 203/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 204/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 205/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 206/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 207/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 208/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 209/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 210/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 211/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 212/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 213/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 214/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 215/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 216/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 217/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 218/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 219/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 220/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 221/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 222/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 223/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 224/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 225/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 226/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 227/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 228/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0035 - val_loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 230/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 231/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 232/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 233/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 234/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 235/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 236/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 237/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 238/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 239/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 240/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 241/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 242/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 243/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 244/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 245/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 246/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 247/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 248/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 249/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 250/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 251/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 252/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 253/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 254/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 255/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 256/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 257/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 258/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 259/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 260/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 261/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 262/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 263/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 264/1050\n",
      "1005/1005 [==============================] - 0s 240us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 265/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 266/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 267/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 182us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 268/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 269/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 270/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 271/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 272/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 273/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 274/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 275/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 276/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 277/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 278/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 279/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 280/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 281/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 282/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 283/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 284/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 285/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 286/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 287/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 288/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 289/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 290/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 291/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 292/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 293/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 294/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 295/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 296/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 297/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 298/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 299/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 300/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 301/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 302/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 303/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 304/1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 305/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 306/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 307/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 308/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 309/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 310/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 311/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 312/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 313/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 314/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 315/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 316/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 317/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 318/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 319/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 320/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 321/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 322/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 323/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 324/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 325/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 326/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 327/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 328/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 329/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 330/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 331/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 332/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 333/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 334/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 335/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 336/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 337/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 338/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 339/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 340/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 341/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 342/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 343/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 344/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 345/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 346/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 347/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 183us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 348/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 349/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 350/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 351/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 352/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 353/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 354/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 355/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 356/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 357/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 358/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 359/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 360/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 361/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 362/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 363/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 364/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 365/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 366/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 367/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 368/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 369/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 370/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 371/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 372/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 373/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 374/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 375/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 376/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 377/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 378/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 379/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 381/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 382/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 383/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 384/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 385/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 386/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 387/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 388/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 389/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 390/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 391/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 392/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 393/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 394/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 395/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 396/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 397/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 398/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 399/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 400/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 401/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 402/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 403/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 404/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 405/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 406/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 407/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 408/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 409/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 410/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 411/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 412/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 413/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 414/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 415/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 416/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 417/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 418/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 419/1050\n",
      "1005/1005 [==============================] - 0s 158us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 420/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 421/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 422/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 423/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 424/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 425/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 426/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 427/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 428/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 429/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 430/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 431/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 432/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 433/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 434/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 435/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 436/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 437/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 438/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 439/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 440/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 441/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 442/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 443/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 444/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 445/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 446/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 447/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 448/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 449/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 450/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 451/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 452/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 453/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 454/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 455/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0032 - val_loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 457/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 458/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 459/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 460/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 461/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 462/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 463/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 464/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 465/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 466/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 467/1050\n",
      "1005/1005 [==============================] - 0s 364us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 468/1050\n",
      "1005/1005 [==============================] - 0s 427us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 469/1050\n",
      "1005/1005 [==============================] - 0s 363us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 470/1050\n",
      "1005/1005 [==============================] - 0s 246us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 471/1050\n",
      "1005/1005 [==============================] - 0s 242us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 472/1050\n",
      "1005/1005 [==============================] - 0s 329us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 473/1050\n",
      "1005/1005 [==============================] - 0s 363us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 474/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 475/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 476/1050\n",
      "1005/1005 [==============================] - 0s 230us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 477/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 478/1050\n",
      "1005/1005 [==============================] - 0s 258us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 479/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 480/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 481/1050\n",
      "1005/1005 [==============================] - 0s 255us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 482/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 483/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 484/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 485/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 486/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 487/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 488/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 489/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 490/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 491/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 492/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 493/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 494/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 495/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 496/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 497/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 498/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 499/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 500/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 501/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 502/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 503/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 504/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 505/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 506/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 507/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 508/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 509/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 510/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 511/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 512/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 513/1050\n",
      "1005/1005 [==============================] - 0s 435us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 514/1050\n",
      "1005/1005 [==============================] - 0s 430us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 515/1050\n",
      "1005/1005 [==============================] - 1s 524us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 516/1050\n",
      "1005/1005 [==============================] - 0s 332us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 517/1050\n",
      "1005/1005 [==============================] - 0s 317us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 518/1050\n",
      "1005/1005 [==============================] - 0s 296us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 519/1050\n",
      "1005/1005 [==============================] - 0s 354us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 520/1050\n",
      "1005/1005 [==============================] - 0s 247us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 521/1050\n",
      "1005/1005 [==============================] - 0s 285us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 522/1050\n",
      "1005/1005 [==============================] - 0s 235us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 523/1050\n",
      "1005/1005 [==============================] - 0s 256us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 524/1050\n",
      "1005/1005 [==============================] - 0s 260us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 525/1050\n",
      "1005/1005 [==============================] - 0s 302us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 526/1050\n",
      "1005/1005 [==============================] - 0s 320us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 527/1050\n",
      "1005/1005 [==============================] - 0s 259us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 528/1050\n",
      "1005/1005 [==============================] - 0s 259us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 529/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 530/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 531/1050\n",
      "1005/1005 [==============================] - 0s 225us/step - loss: 0.0032 - val_loss: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1050\n",
      "1005/1005 [==============================] - 0s 248us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 533/1050\n",
      "1005/1005 [==============================] - 0s 281us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 534/1050\n",
      "1005/1005 [==============================] - 0s 264us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 535/1050\n",
      "1005/1005 [==============================] - 0s 261us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 536/1050\n",
      "1005/1005 [==============================] - 0s 262us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 537/1050\n",
      "1005/1005 [==============================] - 0s 244us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 538/1050\n",
      "1005/1005 [==============================] - 0s 258us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 539/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 540/1050\n",
      "1005/1005 [==============================] - 0s 256us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 541/1050\n",
      "1005/1005 [==============================] - 0s 236us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 542/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 543/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 544/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 545/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 546/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 547/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 548/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 549/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 550/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 551/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 552/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 553/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 554/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 555/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 556/1050\n",
      "1005/1005 [==============================] - 0s 283us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 557/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 558/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 559/1050\n",
      "1005/1005 [==============================] - 0s 277us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 560/1050\n",
      "1005/1005 [==============================] - 0s 261us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 561/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 562/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 563/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 564/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 565/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 566/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 567/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 568/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 569/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 570/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 571/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 572/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 573/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 574/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 575/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 576/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 577/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 578/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 579/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 580/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 581/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 582/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 583/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 584/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 585/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 586/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 587/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 588/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 589/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 590/1050\n",
      "1005/1005 [==============================] - 0s 290us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 591/1050\n",
      "1005/1005 [==============================] - 0s 276us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 592/1050\n",
      "1005/1005 [==============================] - 0s 266us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 593/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 594/1050\n",
      "1005/1005 [==============================] - 0s 248us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 595/1050\n",
      "1005/1005 [==============================] - 0s 247us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 596/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 597/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 598/1050\n",
      "1005/1005 [==============================] - 0s 237us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 599/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 600/1050\n",
      "1005/1005 [==============================] - 0s 250us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 601/1050\n",
      "1005/1005 [==============================] - 0s 395us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 602/1050\n",
      "1005/1005 [==============================] - 0s 232us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 603/1050\n",
      "1005/1005 [==============================] - 0s 242us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 604/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 605/1050\n",
      "1005/1005 [==============================] - 0s 251us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 606/1050\n",
      "1005/1005 [==============================] - 0s 236us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 607/1050\n",
      "1005/1005 [==============================] - 0s 232us/step - loss: 0.0031 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 609/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 610/1050\n",
      "1005/1005 [==============================] - 0s 238us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 611/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 612/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 613/1050\n",
      "1005/1005 [==============================] - 0s 223us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 614/1050\n",
      "1005/1005 [==============================] - 0s 257us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 615/1050\n",
      "1005/1005 [==============================] - 0s 313us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 616/1050\n",
      "1005/1005 [==============================] - 0s 272us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 617/1050\n",
      "1005/1005 [==============================] - 0s 262us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 618/1050\n",
      "1005/1005 [==============================] - 0s 230us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 619/1050\n",
      "1005/1005 [==============================] - 0s 267us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 620/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 621/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 622/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 623/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 624/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 625/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 626/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 627/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 628/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 629/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 630/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 631/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 632/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 633/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 634/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 635/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 636/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 637/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 638/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 639/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 640/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 641/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 642/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 643/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 644/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 645/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 646/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 647/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 648/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 649/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 650/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 651/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 652/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 653/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 654/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 655/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 656/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 657/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 658/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 659/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 660/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 661/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 662/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 663/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 664/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 665/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 666/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 667/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 668/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 669/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 670/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 671/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 672/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 673/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 674/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 675/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 676/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 677/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 678/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 679/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 680/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 681/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 682/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 683/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 685/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 686/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 687/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 688/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 689/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 690/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 691/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 692/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 693/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 694/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 695/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 696/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 697/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 698/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 699/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 700/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 701/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 702/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 703/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 704/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 705/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 706/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 707/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 708/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 709/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 710/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 711/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 712/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 713/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 714/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 715/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 716/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 717/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 718/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 719/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 720/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 721/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 722/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 723/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 724/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 725/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 726/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 727/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 728/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 729/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 730/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 731/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 732/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 733/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 734/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 735/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 736/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 737/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 738/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 739/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 740/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 741/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 742/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 743/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 744/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 745/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 746/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 747/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 748/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 749/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 750/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 751/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 752/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 753/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 754/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 755/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 756/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 757/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 758/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 759/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 761/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 762/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 763/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 764/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 765/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 766/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 767/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 768/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 769/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 770/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 771/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 772/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 773/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 774/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 775/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 776/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 777/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 778/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 779/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 780/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 781/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 782/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 783/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 784/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 785/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 786/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 787/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 788/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 789/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 790/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 791/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 792/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 793/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 794/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 795/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 796/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 797/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 798/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 799/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 800/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 801/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 802/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 803/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 804/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 805/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 806/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 807/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 808/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 809/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 810/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 811/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 812/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 813/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 814/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 815/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 816/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 817/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 818/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 819/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 820/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 821/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 822/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 823/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 824/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 825/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 826/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 827/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 828/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 829/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 830/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 831/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 832/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 833/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 834/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 835/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 837/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 838/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 839/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 840/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 841/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 842/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 843/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 844/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 845/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 846/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 847/1050\n",
      "1005/1005 [==============================] - 0s 251us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 848/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 849/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 850/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 851/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 852/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 853/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 854/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 855/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 856/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 857/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 858/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 859/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 860/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 861/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 862/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 863/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 864/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 865/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 866/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 867/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 868/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 869/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 870/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 871/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 872/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 873/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 874/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 875/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 876/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 877/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 878/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 879/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 880/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 881/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 882/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 883/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 884/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 885/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 886/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 887/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 888/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 889/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 890/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 891/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 892/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 893/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 894/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 895/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 896/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 897/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 898/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 899/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 900/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 901/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 902/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 903/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 904/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 905/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 906/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 907/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 908/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 909/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 910/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 911/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 912/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 913/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 914/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 915/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 916/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 917/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 918/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 919/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 920/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 921/1050\n",
      "1005/1005 [==============================] - 0s 237us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 922/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 923/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 924/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 925/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 926/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 927/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 928/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 929/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 930/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 931/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 932/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 933/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 934/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 935/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 936/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 937/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 938/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 939/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 940/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 941/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 942/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 943/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 944/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 945/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 946/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 947/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 948/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 949/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 950/1050\n",
      "1005/1005 [==============================] - 0s 233us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 951/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 952/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 953/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 954/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 955/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 956/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 957/1050\n",
      "1005/1005 [==============================] - 0s 241us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 958/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 959/1050\n",
      "1005/1005 [==============================] - 0s 240us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 960/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 961/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 962/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 963/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 964/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 965/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 966/1050\n",
      "1005/1005 [==============================] - 0s 134us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 967/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 968/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 969/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 970/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 971/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 972/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 973/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 974/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 975/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 976/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 977/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 978/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 979/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 980/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 981/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 982/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 983/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 984/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 985/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 986/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 987/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 988/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 989/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 990/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 991/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 992/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 993/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 994/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 995/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 996/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 997/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 998/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 999/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1000/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1001/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1002/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1003/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1004/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1005/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1006/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1007/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1008/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1009/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1010/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1011/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1012/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1013/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 1014/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1015/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1016/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1017/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1018/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1019/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1020/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1021/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1022/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 1023/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1024/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 1025/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1026/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1027/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1028/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.002 - 0s 183us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1029/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1030/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1031/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1032/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1033/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1034/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1035/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1036/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1037/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1038/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1039/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1040/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1041/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1042/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1043/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1044/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1045/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1046/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1047/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1048/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1049/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1050/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0028 - val_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=1050,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split=0.25 )\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.03417015, ..., 0.        , 0.99791493,\n",
       "        0.97710126],\n",
       "       [0.13043478, 0.        , 0.05421897, ..., 0.        , 0.0675563 ,\n",
       "        0.97680223],\n",
       "       [0.01449275, 0.        , 0.00523013, ..., 0.        , 0.03211009,\n",
       "        0.97702075],\n",
       "       ...,\n",
       "       [0.13043478, 0.        , 0.04890167, ..., 0.        , 0.93911593,\n",
       "        0.9775383 ],\n",
       "       [0.13043478, 0.        , 0.07705718, ..., 0.        , 0.99541284,\n",
       "        0.97775682],\n",
       "       [0.13043478, 0.04761905, 0.04279986, ..., 0.        , 0.23853211,\n",
       "        0.97736578]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolor = ['r', 'g', 'b']\\nmarker = ['s', 'x', 'o']\\nfor l, c, m in zip(np.unique(y_train), color, marker):\\n    plt.scatter(x_train_pca[y_train == l, 0],\\n    x_train_pca[y_train == l, 1],\\n    c=c, label=l, marker=m)\\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\\nplt.title('Result')\\nplt.xlabel('PC1')\\nplt.ylabel('PC2')\\nplt.legend(loc='upper left')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot3clusters(x_train,title,vtitle):\n",
    "    plt.figure()\n",
    "    colors = ['navy','turquoise','darkorange']\n",
    "    marker = ['s', 'x', 'o']\n",
    "    lw = 2 \n",
    "    for c,m,i,col in zip(colors,marker,np.unique(y_train),x_data.columns.tolist()):\n",
    "        plt.scatter(x_train[y_train == i, 0],\n",
    "        x_train[y_train == i, 1],\n",
    "        c=c, label=col, marker=m)\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "    plt.legend(loc = 'best',shadow=False ,scatterpoints = 1)\n",
    "    plt.title('PCA')\n",
    "    plt.xlabel(vtitle + '1')\n",
    "    plt.ylabel(vtitle + '2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "color = ['r', 'g', 'b']\n",
    "marker = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), color, marker):\n",
    "    plt.scatter(x_train_pca[y_train == l, 0],\n",
    "    x_train_pca[y_train == l, 1],\n",
    "    c=c, label=l, marker=m)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "plt.title('Result')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca_transformed = pca.fit_transform(x_train)\n",
    "plot3clusters(pca_transformed[:,:2],'PCA','PC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "油耗量（当天）    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构建一个神经元-所有特征做训练\n",
    "#两隐藏层，一个100神经元，一个50神经元，epoch  500次\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (100,50),max_iter = 500)\n",
    "y_train = pd.DataFrame(y_train.fillna(0))\n",
    "y_train = y_train.astype('int')\n",
    "print(y_train.isnull().any())\n",
    "mlp.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       178\n",
      "           1       0.00      0.25      0.01         4\n",
      "           3       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         6\n",
      "           8       0.00      0.00      0.00         9\n",
      "           9       0.00      0.00      0.00        20\n",
      "          10       0.00      0.00      0.00         6\n",
      "          11       0.00      0.00      0.00        14\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00        15\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.00      0.00      0.00        10\n",
      "          17       0.00      0.00      0.00         4\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         3\n",
      "          22       0.00      0.00      0.00         9\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.00      0.00      0.00        15\n",
      "          29       0.00      0.00      0.00         9\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          34       0.00      0.00      0.00         2\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         9\n",
      "          37       0.00      0.00      0.00        16\n",
      "          38       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00        13\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00        10\n",
      "          50       0.00      0.00      0.00         5\n",
      "          52       0.00      0.00      0.00         9\n",
      "          53       0.00      0.00      0.00        13\n",
      "          64       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.00      0.00      0.00         3\n",
      "         112       0.00      0.00      0.00         2\n",
      "         130       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.40       447\n",
      "   macro avg       0.02      0.03      0.02       447\n",
      "weighted avg       0.37      0.40      0.38       447\n",
      "\n",
      "[[178   0   0 ...   0   0   0]\n",
      " [  3   1   0 ...   0   0   0]\n",
      " [  1   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   1   0 ...   0   0   0]\n",
      " [  0   1   0 ...   0   0   0]\n",
      " [  0   1   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#做测试\n",
    "#预测结果\n",
    "predictions = mlp.predict(x_test)\n",
    "print(type(predictions))\n",
    "print(type(y_test))\n",
    "#预测结果和标签对比\n",
    "print(classification_report(predictions,y_test> 0.5))\n",
    "print(confusion_matrix(predictions,y_test> 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
