{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import sklearn.preprocessing as pre_processing\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')\n",
    "\n",
    "#载入维度数据\n",
    "x_data = datasets[[ '吨位', '当日有派车次数',\n",
    "                   '全天行驶里程km', '其中任务行驶里程km', '全天运行时长（点火时长）hh', '其中任务行驶时长hh', '全天静驶时长',\n",
    "                   '平均时速km/h', '最大扭距', '平均扭距', '急刹车次数', '超速报警次数', '前3日平均运行里程km', '前3日日平均运行时长h',\n",
    "                   '近4日平均运行里程km', '近4日日平均运行时长h', '怠速次数', '怠速时长mm', '任务百公里油耗（L）', '全里程百公里油耗（L）'\n",
    "                   , '高档低速次数', '高档低速累计时长mm', '低档高速次数', '低档高速累计时长mm', '空档滑行次数', '空档滑行时长mm',\n",
    "                   'Can设备状态',  '市趟次数', '一干次数', '二干次数', '里程标杆值km',\n",
    "                  '实际里程参考值km', '位置总数', '未锁星数', '延迟位置数',\n",
    "                   '行驶位置数', '间隔大位置数', 'ACC关位置数',  '行驶时长', '拉直线次数', '拉直线总距离', \n",
    "                   '设备类型', '在线时长', '里程误差']]\n",
    "x_data = x_data.replace([np.inf, -np.inf], np.nan)\n",
    "x_data = x_data.fillna(0)\n",
    "\n",
    "#载入标签数据\n",
    "y_data = datasets['油耗量（当天）']\n",
    "\n",
    "#分割数据1/4为测试，3/4为训练数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import sklearn.preprocessing as pre_processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib.font_manager import *\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 44)\n",
      "(447, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 44)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                1620      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                888       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 125       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 14)                42        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                300       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                630       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 44)                1364      \n",
      "=================================================================\n",
      "Total params: 4,981\n",
      "Trainable params: 4,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1005 samples, validate on 335 samples\n",
      "Epoch 1/50\n",
      "1005/1005 [==============================] - 1s 544us/step - loss: 0.0366 - val_loss: 0.0185\n",
      "Epoch 2/50\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0152 - val_loss: 0.0124\n",
      "Epoch 3/50\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 4/50\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 5/50\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0098 - val_loss: 0.0092\n",
      "Epoch 6/50\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0094 - val_loss: 0.0090\n",
      "Epoch 7/50\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 8/50\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 9/50\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 10/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0083 - val_loss: 0.0078\n",
      "Epoch 11/50\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 12/50\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0078 - val_loss: 0.0073\n",
      "Epoch 13/50\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 14/50\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 15/50\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 16/50\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 17/50\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 18/50\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 19/50\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 20/50\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 21/50\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 22/50\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 23/50\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 24/50\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 25/50\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 26/50\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 27/50\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 28/50\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 29/50\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 30/50\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 31/50\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 32/50\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 33/50\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 34/50\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 35/50\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 36/50\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 37/50\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 38/50\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 39/50\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 40/50\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 41/50\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 42/50\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 43/50\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 44/50\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 45/50\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 46/50\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 47/50\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 48/50\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 49/50\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 50/50\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0054 - val_loss: 0.0050\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# please note, all tutorial code are running under python3.5.\n",
    "# If you use the version like python2.7, please modify the code accordingly\n",
    "\n",
    "# 9 - Autoencoder example\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# to try tensorflow, un-comment following two lines\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called\n",
    "# X shape (60,000 28x28), y shape (10,000, )\n",
    "datasets = pd.read_excel('..\\【071】单车日报表.xlsx')\n",
    "datasets = datasets.drop(datasets.loc[(datasets['吨位'] == 0 )| (datasets['全天行驶里程km'] == 0 ) ].index)\n",
    "#载入维度数据\n",
    "x_data = datasets[[ '吨位', '当日有派车次数',\n",
    "                   '全天行驶里程km', '其中任务行驶里程km', '全天运行时长（点火时长）hh', '其中任务行驶时长hh', '全天静驶时长',\n",
    "                   '平均时速km/h', '最大扭距', '平均扭距', '急刹车次数', '超速报警次数', '前3日平均运行里程km', '前3日日平均运行时长h',\n",
    "                   '近4日平均运行里程km', '近4日日平均运行时长h', '怠速次数', '怠速时长mm', '任务百公里油耗（L）', '全里程百公里油耗（L）'\n",
    "                   , '高档低速次数', '高档低速累计时长mm', '低档高速次数', '低档高速累计时长mm', '空档滑行次数', '空档滑行时长mm',\n",
    "                   'Can设备状态',  '市趟次数', '一干次数', '二干次数', '里程标杆值km',\n",
    "                  '实际里程参考值km', '位置总数', '未锁星数', '延迟位置数',\n",
    "                   '行驶位置数', '间隔大位置数', 'ACC关位置数',  '行驶时长', '拉直线次数', '拉直线总距离', \n",
    "                   '设备类型', '在线时长', '里程误差']]\n",
    "x_data = x_data.replace([np.inf, -np.inf], np.nan)\n",
    "x_data = x_data.fillna(0)\n",
    "\n",
    "\n",
    "#载入标签数据\n",
    "y_data = datasets['油耗量（当天）']\n",
    "\n",
    "#分割数据1/4为测试，3/4为训练数据\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_data,y_data)\n",
    "\n",
    "\n",
    "#(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "'''\n",
    "# data pre-processing\n",
    "x_train = x_train.astype('float32') / 255. - 0.5       # minmax_normalized\n",
    "x_test = x_test.astype('float32') / 255. - 0.5         # minmax_normalized\n",
    "\n",
    "# standardize the feature 标准化\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "'''\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# in order to plot in a 2D figure\n",
    "encoding_dim = 2\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(44,))\n",
    "\n",
    "# encoder layers\n",
    "encoded = Dense(36, activation='relu')(input_img)\n",
    "encoded = Dense(24, activation='relu')(encoded)\n",
    "encoded = Dense(5, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    "\n",
    "# decoder layers\n",
    "decoded = Dense(14, activation='relu')(encoder_output)\n",
    "decoded = Dense(20, activation='relu')(decoded)\n",
    "decoded = Dense(30, activation='relu')(decoded)\n",
    "decoded = Dense(44, activation='tanh')(decoded)\n",
    "\n",
    "# construct the autoencoder model\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "# construct the encoder model for plotting\n",
    "encoder = Model(input=input_img, output=encoder_output)\n",
    "\n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "print(autoencoder.summary())\n",
    "\n",
    "# training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split=0.25 )\n",
    "\n",
    "#print(history.history.keys()) # 可查看 history 对象中有哪些历史数据\n",
    "\n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1005 samples, validate on 335 samples\n",
      "Epoch 1/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 2/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 3/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 4/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 5/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 6/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 7/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 8/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 9/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 10/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 11/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 12/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 13/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 14/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 15/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0054 - val_loss: 0.0052\n",
      "Epoch 16/1050\n",
      "1005/1005 [==============================] - 0s 246us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 17/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 18/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 19/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 20/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 21/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 22/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 23/1050\n",
      "1005/1005 [==============================] - 0s 229us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 24/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 25/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 26/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 27/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 28/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 29/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 30/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 31/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 32/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 33/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 34/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 35/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 36/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 37/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 38/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 39/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 40/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 41/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 42/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 43/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 44/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 45/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 46/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 47/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 48/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 49/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 50/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 51/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 52/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 53/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 54/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 55/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 56/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 57/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 58/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 59/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 60/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 61/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 62/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 63/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 64/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 65/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 66/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 67/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 68/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 69/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 70/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 71/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 72/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 73/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 74/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 75/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 76/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 77/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 78/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 79/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 80/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 81/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 82/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 83/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 84/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 85/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 86/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 87/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 88/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 89/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 90/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 91/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 92/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 93/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 94/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 95/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 96/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0040 - val_loss: 0.0038\n",
      "Epoch 97/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 98/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 99/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 100/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 101/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 102/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 103/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 104/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 105/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 106/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 107/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 108/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 109/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 110/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 111/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 112/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 113/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 114/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 115/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 116/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 117/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 118/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 119/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 120/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 121/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 122/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 123/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 124/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 125/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 126/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 127/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 128/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 129/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 130/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 131/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 132/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 133/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 134/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 135/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 136/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 137/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 138/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 139/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 140/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 141/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 142/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 143/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 144/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 145/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 146/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 147/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 148/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 149/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 150/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 151/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 152/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 153/1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 154/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 155/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 156/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 157/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 158/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 159/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 160/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 161/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 162/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 163/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 164/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 165/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 166/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 167/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 168/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 169/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 170/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 171/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 172/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 173/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 174/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 175/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 176/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 177/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 178/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 179/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 180/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 181/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 182/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 183/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 184/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 185/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 186/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 187/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 188/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 189/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 190/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 191/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 192/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 193/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 194/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 195/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 196/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 197/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 198/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 199/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 200/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 201/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 202/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 203/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 204/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 205/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 206/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 207/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 208/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 209/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 210/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 211/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 212/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 213/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 214/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 215/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 216/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 217/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 218/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 219/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 220/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 221/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 222/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 223/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 224/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 225/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 226/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 227/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 228/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0035 - val_loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 230/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 231/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 232/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 233/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 234/1050\n",
      "1005/1005 [==============================] - 0s 167us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 235/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 236/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 237/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 238/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 239/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 240/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 241/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 242/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 243/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 244/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 245/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 246/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 247/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 248/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 249/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 250/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 251/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 252/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 253/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 254/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 255/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 256/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 257/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 258/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 259/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 260/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 261/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 262/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 263/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 264/1050\n",
      "1005/1005 [==============================] - 0s 240us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 265/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 266/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 267/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 182us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 268/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 269/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 270/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 271/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 272/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 273/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 274/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 275/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 276/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 277/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 278/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 279/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 280/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 281/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 282/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 283/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 284/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 285/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 286/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 287/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 288/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 289/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 290/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 291/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 292/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 293/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 294/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 295/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 296/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 297/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 298/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 299/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 300/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 301/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 302/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 303/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 304/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 305/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 306/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 307/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 308/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 309/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 310/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 311/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 312/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 313/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 314/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 315/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 316/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 317/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 318/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 319/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 320/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 321/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 322/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 323/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 324/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 325/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 326/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 327/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 328/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 329/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 330/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 331/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 332/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 333/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 334/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 335/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 336/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 337/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 338/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 339/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 340/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 341/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 342/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 343/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 344/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 345/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 346/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 347/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 183us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 348/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 349/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 350/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 351/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 352/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 353/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 354/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 355/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 356/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 357/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 358/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 359/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 360/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 361/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 362/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 363/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 364/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 365/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 366/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 367/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 368/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 369/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 370/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 371/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 372/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 373/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 374/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 375/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 376/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 377/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 378/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 379/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.003 - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1050\n",
      "1005/1005 [==============================] - 0s 168us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 381/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 382/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 383/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 384/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 385/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 386/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 387/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 388/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 389/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 390/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 391/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 392/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 393/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 394/1050\n",
      "1005/1005 [==============================] - 0s 165us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 395/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 396/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 397/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 398/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 399/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 400/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 401/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 402/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 403/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 404/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 405/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 406/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 407/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 408/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 409/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 410/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 411/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 412/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 413/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 414/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 415/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 416/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 417/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 418/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 419/1050\n",
      "1005/1005 [==============================] - 0s 158us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 420/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 421/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 422/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 423/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 424/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 425/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 426/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 427/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 428/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 429/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 430/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 431/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 432/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 433/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 434/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 435/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 436/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 437/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 438/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 439/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 440/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 441/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 442/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 443/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 444/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 445/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 446/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 447/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 448/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 449/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 450/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 451/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 452/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 453/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 454/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 455/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0032 - val_loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 457/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 458/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 459/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 460/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 461/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 462/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 463/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 464/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 465/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 466/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 467/1050\n",
      "1005/1005 [==============================] - 0s 364us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 468/1050\n",
      "1005/1005 [==============================] - 0s 427us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 469/1050\n",
      "1005/1005 [==============================] - 0s 363us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 470/1050\n",
      "1005/1005 [==============================] - 0s 246us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 471/1050\n",
      "1005/1005 [==============================] - 0s 242us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 472/1050\n",
      "1005/1005 [==============================] - 0s 329us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 473/1050\n",
      "1005/1005 [==============================] - 0s 363us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 474/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 475/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 476/1050\n",
      "1005/1005 [==============================] - 0s 230us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 477/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 478/1050\n",
      "1005/1005 [==============================] - 0s 258us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 479/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 480/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 481/1050\n",
      "1005/1005 [==============================] - 0s 255us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 482/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 483/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 484/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 485/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 486/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 487/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 488/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 489/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 490/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 491/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 492/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 493/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 494/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 495/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 496/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 497/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 498/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 499/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 500/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 501/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 502/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 503/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 504/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 505/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 506/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 507/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 508/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 509/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 510/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 511/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 512/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 513/1050\n",
      "1005/1005 [==============================] - 0s 435us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 514/1050\n",
      "1005/1005 [==============================] - 0s 430us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 515/1050\n",
      "1005/1005 [==============================] - 1s 524us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 516/1050\n",
      "1005/1005 [==============================] - 0s 332us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 517/1050\n",
      "1005/1005 [==============================] - 0s 317us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 518/1050\n",
      "1005/1005 [==============================] - 0s 296us/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 519/1050\n",
      "1005/1005 [==============================] - 0s 354us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 520/1050\n",
      "1005/1005 [==============================] - 0s 247us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 521/1050\n",
      "1005/1005 [==============================] - 0s 285us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 522/1050\n",
      "1005/1005 [==============================] - 0s 235us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 523/1050\n",
      "1005/1005 [==============================] - 0s 256us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 524/1050\n",
      "1005/1005 [==============================] - 0s 260us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 525/1050\n",
      "1005/1005 [==============================] - 0s 302us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 526/1050\n",
      "1005/1005 [==============================] - 0s 320us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 527/1050\n",
      "1005/1005 [==============================] - 0s 259us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 528/1050\n",
      "1005/1005 [==============================] - 0s 259us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 529/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 530/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 531/1050\n",
      "1005/1005 [==============================] - 0s 225us/step - loss: 0.0032 - val_loss: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1050\n",
      "1005/1005 [==============================] - 0s 248us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 533/1050\n",
      "1005/1005 [==============================] - 0s 281us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 534/1050\n",
      "1005/1005 [==============================] - 0s 264us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 535/1050\n",
      "1005/1005 [==============================] - 0s 261us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 536/1050\n",
      "1005/1005 [==============================] - 0s 262us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 537/1050\n",
      "1005/1005 [==============================] - 0s 244us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 538/1050\n",
      "1005/1005 [==============================] - 0s 258us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 539/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 540/1050\n",
      "1005/1005 [==============================] - 0s 256us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 541/1050\n",
      "1005/1005 [==============================] - 0s 236us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 542/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 543/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 544/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 545/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 546/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 547/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 548/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 549/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 550/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 551/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 552/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 553/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 554/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 555/1050\n",
      "1005/1005 [==============================] - 0s 269us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 556/1050\n",
      "1005/1005 [==============================] - 0s 283us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 557/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 558/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 559/1050\n",
      "1005/1005 [==============================] - 0s 277us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 560/1050\n",
      "1005/1005 [==============================] - 0s 261us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 561/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 562/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 563/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 564/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 565/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 566/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 567/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 568/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 569/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 570/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 571/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 572/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 573/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 574/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 575/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 576/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 577/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 578/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 579/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 580/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 581/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 582/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 583/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 584/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 585/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 586/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 587/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 588/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 589/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 590/1050\n",
      "1005/1005 [==============================] - 0s 290us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 591/1050\n",
      "1005/1005 [==============================] - 0s 276us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 592/1050\n",
      "1005/1005 [==============================] - 0s 266us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 593/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 594/1050\n",
      "1005/1005 [==============================] - 0s 248us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 595/1050\n",
      "1005/1005 [==============================] - 0s 247us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 596/1050\n",
      "1005/1005 [==============================] - 0s 243us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 597/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 598/1050\n",
      "1005/1005 [==============================] - 0s 237us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 599/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 600/1050\n",
      "1005/1005 [==============================] - 0s 250us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 601/1050\n",
      "1005/1005 [==============================] - 0s 395us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 602/1050\n",
      "1005/1005 [==============================] - 0s 232us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 603/1050\n",
      "1005/1005 [==============================] - 0s 242us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 604/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 605/1050\n",
      "1005/1005 [==============================] - 0s 251us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 606/1050\n",
      "1005/1005 [==============================] - 0s 236us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 607/1050\n",
      "1005/1005 [==============================] - 0s 232us/step - loss: 0.0031 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 609/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 610/1050\n",
      "1005/1005 [==============================] - 0s 238us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 611/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 612/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 613/1050\n",
      "1005/1005 [==============================] - 0s 223us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 614/1050\n",
      "1005/1005 [==============================] - 0s 257us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 615/1050\n",
      "1005/1005 [==============================] - 0s 313us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 616/1050\n",
      "1005/1005 [==============================] - 0s 272us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 617/1050\n",
      "1005/1005 [==============================] - 0s 262us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 618/1050\n",
      "1005/1005 [==============================] - 0s 230us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 619/1050\n",
      "1005/1005 [==============================] - 0s 267us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 620/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 621/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 622/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 623/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 624/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 625/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 626/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 627/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 628/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 629/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 630/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 631/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 632/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 633/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 634/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 635/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 636/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 637/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 638/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 639/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 640/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 641/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 642/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 643/1050\n",
      "1005/1005 [==============================] - 0s 219us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 644/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 645/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 646/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 647/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 648/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 649/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 650/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 651/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 652/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 653/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 654/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 655/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 656/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 657/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 658/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 659/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 660/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 661/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 662/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 663/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 664/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 665/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 666/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 667/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 668/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 669/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 670/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 671/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 672/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 673/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 674/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 675/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 676/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 677/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 678/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 679/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 680/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 681/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 682/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 683/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 685/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 686/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 687/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 688/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 689/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 690/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 691/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 692/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 693/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 694/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 695/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 696/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 697/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 698/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 699/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 700/1050\n",
      "1005/1005 [==============================] - 0s 221us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 701/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 702/1050\n",
      "1005/1005 [==============================] - 0s 222us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 703/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 704/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 705/1050\n",
      "1005/1005 [==============================] - 0s 211us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 706/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 707/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 708/1050\n",
      "1005/1005 [==============================] - 0s 208us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 709/1050\n",
      "1005/1005 [==============================] - 0s 215us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 710/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 711/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 712/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 713/1050\n",
      "1005/1005 [==============================] - 0s 201us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 714/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 715/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 716/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 717/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 718/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 719/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 720/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 721/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 722/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 723/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 724/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 725/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 726/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 727/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 728/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 729/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 730/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 731/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 732/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 733/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 734/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 735/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 736/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 737/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 738/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 739/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 740/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 741/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 742/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 743/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 744/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 745/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 746/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 747/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 748/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 749/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 750/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 751/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 752/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 753/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 754/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 755/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 756/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 757/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 758/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 759/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 761/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 762/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 763/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 764/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 765/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 766/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 767/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 768/1050\n",
      "1005/1005 [==============================] - 0s 164us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 769/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 770/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 771/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 772/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 773/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 774/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 775/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 776/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 777/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 778/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 779/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 780/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 781/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 782/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 783/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 784/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 785/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 786/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 787/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 788/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 789/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 790/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 791/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 792/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 793/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 794/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 795/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 796/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 797/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 798/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 799/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 800/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 801/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 802/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 803/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 804/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 805/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 806/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 807/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 808/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 809/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 810/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 811/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 812/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 813/1050\n",
      "1005/1005 [==============================] - 0s 220us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 814/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 815/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 816/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 817/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 818/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 819/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 820/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 821/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 822/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 823/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 824/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 825/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 826/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 827/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 828/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 829/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 830/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 831/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 832/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 833/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 834/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 835/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 837/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 838/1050\n",
      "1005/1005 [==============================] - 0s 169us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 839/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 840/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 841/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 842/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 843/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 844/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 845/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 846/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 847/1050\n",
      "1005/1005 [==============================] - 0s 251us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 848/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 849/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 850/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 851/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 852/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 853/1050\n",
      "1005/1005 [==============================] - 0s 205us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 854/1050\n",
      "1005/1005 [==============================] - 0s 166us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 855/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 856/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 857/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 858/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 859/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 860/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 861/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 862/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 863/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 864/1050\n",
      "1005/1005 [==============================] - 0s 206us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 865/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 866/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 867/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 868/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 869/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 870/1050\n",
      "1005/1005 [==============================] - 0s 180us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 871/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 872/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 873/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 874/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 875/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 876/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 877/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 878/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 879/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 880/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 881/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 882/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 883/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 884/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 885/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 886/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 887/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 888/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 889/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 890/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 891/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 892/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 893/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 894/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 895/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 896/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 897/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 898/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 899/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 900/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 901/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 902/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 903/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 904/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 905/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 906/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 907/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 908/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 909/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 910/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 911/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 912/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 913/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 914/1050\n",
      "1005/1005 [==============================] - 0s 174us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 915/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 916/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 917/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 918/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 919/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 920/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 921/1050\n",
      "1005/1005 [==============================] - 0s 237us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 922/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 923/1050\n",
      "1005/1005 [==============================] - 0s 204us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 924/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 925/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 926/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 927/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 928/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 929/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 930/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 931/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 932/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 933/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 934/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 935/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 936/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 937/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 938/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 939/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 940/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 941/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 942/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 943/1050\n",
      "1005/1005 [==============================] - 0s 179us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 944/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 945/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 946/1050\n",
      "1005/1005 [==============================] - 0s 212us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 947/1050\n",
      "1005/1005 [==============================] - 0s 231us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 948/1050\n",
      "1005/1005 [==============================] - 0s 218us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 949/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 950/1050\n",
      "1005/1005 [==============================] - 0s 233us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 951/1050\n",
      "1005/1005 [==============================] - 0s 216us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 952/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 953/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 954/1050\n",
      "1005/1005 [==============================] - 0s 227us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 955/1050\n",
      "1005/1005 [==============================] - 0s 224us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 956/1050\n",
      "1005/1005 [==============================] - 0s 245us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 957/1050\n",
      "1005/1005 [==============================] - 0s 241us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 958/1050\n",
      "1005/1005 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 959/1050\n",
      "1005/1005 [==============================] - 0s 240us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 960/1050\n",
      "1005/1005 [==============================] - 0s 228us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 961/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 962/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 963/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 964/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 965/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 966/1050\n",
      "1005/1005 [==============================] - 0s 134us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 967/1050\n",
      "1005/1005 [==============================] - 0s 177us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 968/1050\n",
      "1005/1005 [==============================] - 0s 207us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 969/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 970/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 971/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 972/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 973/1050\n",
      "1005/1005 [==============================] - 0s 182us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 974/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 975/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 976/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 977/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 978/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 979/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 980/1050\n",
      "1005/1005 [==============================] - 0s 196us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 981/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 982/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 983/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 984/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 985/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 986/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 987/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 988/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 989/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 990/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 991/1050\n",
      "1005/1005 [==============================] - 0s 170us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 992/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 993/1050\n",
      "1005/1005 [==============================] - 0s 202us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 994/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 995/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 996/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 997/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 998/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 999/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1000/1050\n",
      "1005/1005 [==============================] - 0s 203us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1001/1050\n",
      "1005/1005 [==============================] - 0s 175us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1002/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1003/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1004/1050\n",
      "1005/1005 [==============================] - 0s 181us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1005/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1006/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1007/1050\n",
      "1005/1005 [==============================] - 0s 171us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1008/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1009/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1010/1050\n",
      "1005/1005 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1011/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1012/1050\n",
      "1005/1005 [==============================] - 0s 184us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1013/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 1014/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 1015/1050\n",
      "1005/1005 [==============================] - 0s 176us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1016/1050\n",
      "1005/1005 [==============================] - 0s 172us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1017/1050\n",
      "1005/1005 [==============================] - 0s 173us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1018/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1019/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1020/1050\n",
      "1005/1005 [==============================] - 0s 198us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1021/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1022/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 1023/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1024/1050\n",
      "1005/1005 [==============================] - 0s 200us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 1025/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1026/1050\n",
      "1005/1005 [==============================] - 0s 199us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1027/1050\n",
      "1005/1005 [==============================] - 0s 185us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1028/1050\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.002 - 0s 183us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1029/1050\n",
      "1005/1005 [==============================] - 0s 188us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1030/1050\n",
      "1005/1005 [==============================] - 0s 192us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1031/1050\n",
      "1005/1005 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1032/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1033/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1034/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1035/1050\n",
      "1005/1005 [==============================] - 0s 194us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1036/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1037/1050\n",
      "1005/1005 [==============================] - 0s 195us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1038/1050\n",
      "1005/1005 [==============================] - 0s 186us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1039/1050\n",
      "1005/1005 [==============================] - 0s 190us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1040/1050\n",
      "1005/1005 [==============================] - 0s 193us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1041/1050\n",
      "1005/1005 [==============================] - 0s 187us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1042/1050\n",
      "1005/1005 [==============================] - 0s 191us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1043/1050\n",
      "1005/1005 [==============================] - 0s 178us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1044/1050\n",
      "1005/1005 [==============================] - 0s 183us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1045/1050\n",
      "1005/1005 [==============================] - 0s 214us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1046/1050\n",
      "1005/1005 [==============================] - 0s 210us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1047/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1048/1050\n",
      "1005/1005 [==============================] - 0s 209us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 1049/1050\n",
      "1005/1005 [==============================] - 0s 213us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 1050/1050\n",
      "1005/1005 [==============================] - 0s 217us/step - loss: 0.0028 - val_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=1050,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split=0.25 )\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-39cbc4227da5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#预测结果和标签对比\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \"\"\"\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 91\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.03417015, ..., 0.        , 0.99791493,\n",
       "        0.97710126],\n",
       "       [0.13043478, 0.        , 0.05421897, ..., 0.        , 0.0675563 ,\n",
       "        0.97680223],\n",
       "       [0.01449275, 0.        , 0.00523013, ..., 0.        , 0.03211009,\n",
       "        0.97702075],\n",
       "       ...,\n",
       "       [0.13043478, 0.        , 0.04890167, ..., 0.        , 0.93911593,\n",
       "        0.9775383 ],\n",
       "       [0.13043478, 0.        , 0.07705718, ..., 0.        , 0.99541284,\n",
       "        0.97775682],\n",
       "       [0.13043478, 0.04761905, 0.04279986, ..., 0.        , 0.23853211,\n",
       "        0.97736578]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolor = ['r', 'g', 'b']\\nmarker = ['s', 'x', 'o']\\nfor l, c, m in zip(np.unique(y_train), color, marker):\\n    plt.scatter(x_train_pca[y_train == l, 0],\\n    x_train_pca[y_train == l, 1],\\n    c=c, label=l, marker=m)\\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\\nplt.title('Result')\\nplt.xlabel('PC1')\\nplt.ylabel('PC2')\\nplt.legend(loc='upper left')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot3clusters(x_train,title,vtitle):\n",
    "    plt.figure()\n",
    "    colors = ['navy','turquoise','darkorange']\n",
    "    marker = ['s', 'x', 'o']\n",
    "    lw = 2 \n",
    "    for c,m,i,col ,j in zip(colors,marker,np.unique(y_train),x_data.columns.tolist(), range(0,len(x_data.columns.tolist()))):\n",
    "        plt.scatter(x_train[y_train == i, j],c=c, label=col, marker=m)\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "    plt.legend(loc = 'best',shadow=False ,scatterpoints = 1)\n",
    "    plt.title('PCA')\n",
    "    plt.xlabel(vtitle + '1')\n",
    "    plt.ylabel(vtitle + '2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "color = ['r', 'g', 'b']\n",
    "marker = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), color, marker):\n",
    "    plt.scatter(x_train_pca[y_train == l, 0],\n",
    "    x_train_pca[y_train == l, 1],\n",
    "    c=c, label=l, marker=m)\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 显示中文\n",
    "plt.title('Result')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-65937d21c82e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot3clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_transformed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PCA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-99de5f4aac1d>\u001b[0m in \u001b[0;36mplot3clusters\u001b[1;34m(x_train, title, vtitle)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'font.sans-serif'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'SimHei'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 显示中文\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshadow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mscatterpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca_transformed = pca.fit_transform(x_train)\n",
    "plot3clusters(pca_transformed[:,:2],'PCA','PC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "油耗量（当天）    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构建一个神经元-所有特征做训练\n",
    "#两隐藏层，一个100神经元，一个50神经元，epoch  500次\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (100,50),max_iter = 500)\n",
    "y_train = pd.DataFrame(y_train.fillna(0))\n",
    "y_train = y_train.astype('int')\n",
    "print(y_train.isnull().any())\n",
    "mlp.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       178\n",
      "           1       0.00      0.25      0.01         4\n",
      "           3       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         6\n",
      "           8       0.00      0.00      0.00         9\n",
      "           9       0.00      0.00      0.00        20\n",
      "          10       0.00      0.00      0.00         6\n",
      "          11       0.00      0.00      0.00        14\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00        15\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.00      0.00      0.00        10\n",
      "          17       0.00      0.00      0.00         4\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         3\n",
      "          22       0.00      0.00      0.00         9\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.00      0.00      0.00        15\n",
      "          29       0.00      0.00      0.00         9\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          34       0.00      0.00      0.00         2\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       0.00      0.00      0.00         9\n",
      "          37       0.00      0.00      0.00        16\n",
      "          38       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00        13\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00        10\n",
      "          50       0.00      0.00      0.00         5\n",
      "          52       0.00      0.00      0.00         9\n",
      "          53       0.00      0.00      0.00        13\n",
      "          64       0.00      0.00      0.00         1\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.00      0.00      0.00         3\n",
      "         112       0.00      0.00      0.00         2\n",
      "         130       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         1\n",
      "         215       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.40       447\n",
      "   macro avg       0.02      0.03      0.02       447\n",
      "weighted avg       0.37      0.40      0.38       447\n",
      "\n",
      "[[178   0   0 ...   0   0   0]\n",
      " [  3   1   0 ...   0   0   0]\n",
      " [  1   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   1   0 ...   0   0   0]\n",
      " [  0   1   0 ...   0   0   0]\n",
      " [  0   1   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#做测试\n",
    "#预测结果\n",
    "predictions = mlp.predict(x_test)\n",
    "print(type(predictions))\n",
    "print(type(y_test))\n",
    "#预测结果和标签对比\n",
    "print(classification_report(predictions,y_test> 0.5))\n",
    "print(confusion_matrix(predictions,y_test> 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
